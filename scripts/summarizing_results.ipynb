{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix working directory and Python path to find src module from scripts directory\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get current working directory and manually remove \"scripts\" if present\n",
    "cwd = os.getcwd()\n",
    "print(f\"Original working directory: {cwd}\")\n",
    "\n",
    "# If we're in scripts directory, change to the parent directory\n",
    "if cwd.endswith(\"scripts\"):\n",
    "    project_root = os.path.dirname(cwd)\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "else:\n",
    "    project_root = cwd\n",
    "    print(f\"Already in project root: {project_root}\")\n",
    "\n",
    "# Add project root to Python path if not already present\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Final working directory: {os.getcwd()}\")\n",
    "print(f\"Python path updated. First few entries: {sys.path[:3]}\")\n",
    "\n",
    "## Imports and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_structures import ExperimentConfig, ControlConfig, TreatmentConfig\n",
    "from src.inspect_helpers.tasks import injection_consistency_and_recognition\n",
    "from src.inspect_helpers.datasets import ROW_INDEX_KEY\n",
    "from src.inspect_helpers.scorers import custom_match, custom_prompt_criterion_mgf\n",
    "from src.inspect_helpers.utils import collect_logs_by_model, get_validated_logs_by_model\n",
    "from inspect_ai.log import EvalLog, list_eval_logs, read_eval_log\n",
    "from inspect_ai.model import (\n",
    "    Model,\n",
    "    ModelAPI,\n",
    "    GenerateConfig,\n",
    "    anthropic,\n",
    "    ollama,\n",
    "    get_model,\n",
    ")\n",
    "from inspect_ai import eval, eval_async\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "EXPERIMENT_NAME = \"wikihow_summary_injection_PT_v2_scorer_v2_sonnet_4\"\n",
    "CONTROL_LOG_DIR = f\"logs/{EXPERIMENT_NAME}/control\"\n",
    "TREATMENT_LOG_DIR = f\"logs/{EXPERIMENT_NAME}/treatment\"\n",
    "\n",
    "START_IDX = 0\n",
    "END_IDX = 20\n",
    "\n",
    "\n",
    "MODELS = [\n",
    "    \"anthropic/claude-sonnet-4-20250514\",\n",
    "    #\"anthropic/claude-3-5-haiku-20241022\",\n",
    "    #\"ollama/gemma3:1b-it-q8_0\",\n",
    "    # \"ollama/llama3.2:1b-instruct-q8_0\"\n",
    "]\n",
    "\n",
    "#SCORING_MODEL= \"together/Qwen/Qwen3-235B-A22B-Instruct-2507-tput\"\n",
    "SCORING_MODEL= \"anthropic/claude-3-5-haiku-20241022\"\n",
    "\n",
    "islocal = {\n",
    "    \"ollama\": True,\n",
    "    \"together\": False,\n",
    "    \"anthropic\": False,\n",
    "    \"google\": False,\n",
    "}\n",
    "\n",
    "\n",
    "def split_provider_and_model(model: str) -> str:\n",
    "    return model.split(\"/\")[0], model.split(\"/\")[1]\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE_ARGS = {\n",
    "    \"summary_adjectives\": \"very long and detailed, single-paragraph\",\n",
    "}\n",
    "\n",
    "BATCH_SIZE_LOCAL = 4\n",
    "MAX_CONNECTIONS_API = 100\n",
    "\n",
    "LIMIT = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_safe_path(path: str) -> str:\n",
    "    return path.replace(\":\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.treatments.wikisum_utils import get_WikiSum, get_WikiSum_random\n",
    "\n",
    "df = get_WikiSum(\n",
    "    START_IDX,\n",
    "    END_IDX,\n",
    "    save_path=\"data/\",\n",
    "    splits=[\"train\"],\n",
    "    columns=[\"id\", \"title\", \"text\"],\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make CSVs from the control eval logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying treatments to csv datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'c:/Users/jesse/Documents/Code_Projects/Python/injection-recognition/scripts/logs/wikihow_summary_injection_PT_v2_scorer_v2_sonnet_4/control'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m CONTROL_LOG_DIR = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlogs/wikihow_summary_injection_PT_v2_scorer_v2_sonnet_4/control\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m      3\u001b[39m TREATMENT_LOG_DIR = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlogs/wikihow_summary_injection_PT_v2_scorer_v2_sonnet_4/treatment\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m control_evals_df = \u001b[43mevals_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTROL_LOG_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m treatment_evals_df = evals_df(TREATMENT_LOG_DIR)\n\u001b[32m      7\u001b[39m control_evals_df.columns\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jesse\\anaconda3\\envs\\mars-env\\Lib\\site-packages\\inspect_ai\\analysis\\_dataframe\\evals\\table.py:80\u001b[39m, in \u001b[36mevals_df\u001b[39m\u001b[34m(logs, columns, strict, quiet)\u001b[39m\n\u001b[32m     77\u001b[39m verify_prerequisites()\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# resolve logs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m log_paths = \u001b[43mresolve_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# establish progress\u001b[39;00m\n\u001b[32m     83\u001b[39m quiet = quiet \u001b[38;5;28;01mif\u001b[39;00m quiet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m running_in_notebook()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jesse\\anaconda3\\envs\\mars-env\\Lib\\site-packages\\inspect_ai\\analysis\\_dataframe\\util.py:62\u001b[39m, in \u001b[36mresolve_logs\u001b[39m\u001b[34m(logs)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m log_str \u001b[38;5;129;01min\u001b[39;00m logs_str:\n\u001b[32m     61\u001b[39m     fs = filesystem(log_str)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     info = \u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m info.type == \u001b[33m\"\u001b[39m\u001b[33mdirectory\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     64\u001b[39m         log_paths.extend(\n\u001b[32m     65\u001b[39m             [fi \u001b[38;5;28;01mfor\u001b[39;00m fi \u001b[38;5;129;01min\u001b[39;00m fs.ls(info.name, recursive=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m fi.type == \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     66\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jesse\\anaconda3\\envs\\mars-env\\Lib\\site-packages\\inspect_ai\\_util\\file.py:206\u001b[39m, in \u001b[36mFileSystem.info\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m, **kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]) -> FileInfo:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._file_info(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jesse\\anaconda3\\envs\\mars-env\\Lib\\site-packages\\fsspec\\implementations\\local.py:100\u001b[39m, in \u001b[36mLocalFileSystem.info\u001b[39m\u001b[34m(self, path, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# str or path-like\u001b[39;00m\n\u001b[32m     99\u001b[39m     path = \u001b[38;5;28mself\u001b[39m._strip_protocol(path)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     out = os.stat(path, follow_symlinks=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    101\u001b[39m     link = stat.S_ISLNK(out.st_mode)\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m link:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'c:/Users/jesse/Documents/Code_Projects/Python/injection-recognition/scripts/logs/wikihow_summary_injection_PT_v2_scorer_v2_sonnet_4/control'"
     ]
    }
   ],
   "source": [
    "from inspect_ai.analysis import evals_df\n",
    "CONTROL_LOG_DIR = f\"logs/wikihow_summary_injection_PT_v2_scorer_v2_sonnet_4/control\" \n",
    "TREATMENT_LOG_DIR = f\"logs/wikihow_summary_injection_PT_v2_scorer_v2_sonnet_4/treatment\"\n",
    "\n",
    "control_evals_df = evals_df(CONTROL_LOG_DIR)\n",
    "treatment_evals_df = evals_df(TREATMENT_LOG_DIR)\n",
    "control_evals_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from inspect_ai.log import list_eval_logs\n",
    "from inspect_ai.analysis import evals_df, prepare\n",
    "\n",
    "control_logs = list_eval_logs(CONTROL_LOG_DIR, filter=lambda log: log.status == \"success\")\n",
    "treatment_logs = list_eval_logs(TREATMENT_LOG_DIR, filter=lambda log: log.status == \"success\")\n",
    "\n",
    "control_evals_df = evals_df(control_logs)\n",
    "treatment_evals_df = evals_df(treatment_logs)\n",
    "    \n",
    "control_and_treatments_df = pd.concat([control_evals_df, treatment_evals_df])\n",
    "\n",
    "control_and_treatments_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_and_treatments_df.task_arg_csv_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axes of interest:\n",
    "\n",
    "Bar chart:\n",
    "- Model\n",
    "- Model provider (pattern)\n",
    "- Treatment type (Seperate plots)\n",
    "- Treatment strength (h_concat)\n",
    "- Injection length (0 for control) (v_concat)\n",
    "- Whether injection? Score & stderr (y)\n",
    "- What injection? Score & stderr \n",
    "\n",
    "1. Filter to status = \"success\"\n",
    "2. make separate columns for injection length from task_arg_treatment_col (0 for control evals)\n",
    "3. make separate columns for treatment strength from task_arg_treatment_col\n",
    "4. Make a column for what injection? from task_arg_csv_file_path\n",
    "5. Make a column for whether injection? from injection length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analyzer import Analyzer\n",
    "\n",
    "evals_analyzer = Analyzer(control_and_treatments_df)\n",
    "\n",
    "def get_injection_length(treatment_col : str | None) -> int:\n",
    "    if treatment_col is None or pd.isna(treatment_col):\n",
    "        return 0\n",
    "    return int(treatment_col.split(\"IL\")[1].split(\"_\")[0])\n",
    "\n",
    "def get_treatment_strength(treatment_col : str | None) -> str | None:\n",
    "    if treatment_col is None or pd.isna(treatment_col):\n",
    "        return None\n",
    "    return treatment_col.split(\"_\")[1]\n",
    "\n",
    "def get_treatment_type(file_path : str | None) -> str | None:\n",
    "    if file_path is None or pd.isna(file_path):\n",
    "        return None\n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "    if file_name.startswith(\"dataset_\") and file_name.endswith(\"injected.csv\"):\n",
    "        return file_name.split(\"_\")[1]\n",
    "    return None\n",
    "\n",
    "evals_analyzer.add_column(\n",
    "    column_name=\"has_treatment\",\n",
    "    column_spec = {\n",
    "        \"task_arg_treatment_col\": lambda x : x is not None and not pd.isna(x)\n",
    "    }\n",
    ")\n",
    "\n",
    "evals_analyzer.add_column(\n",
    "    column_name=\"injection_length\",\n",
    "    column_spec = {\n",
    "        \"task_arg_treatment_col\": get_injection_length\n",
    "    }\n",
    ")\n",
    "\n",
    "evals_analyzer.add_column(\n",
    "    column_name=\"treatment_strength\",\n",
    "    column_spec = {\n",
    "        \"task_arg_treatment_col\": get_treatment_strength\n",
    "    }\n",
    ")\n",
    "\n",
    "evals_analyzer.add_column(\n",
    "    column_name=\"treatment_type\",\n",
    "    column_spec = {\n",
    "        \"task_arg_csv_file_path\": get_treatment_type\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Replace S0 with \"light\" and S4 with \"heavy\" in treatment_strength column\n",
    "evals_analyzer.df['treatment_strength'] = evals_analyzer.df['treatment_strength'].replace({\n",
    "    'S0': 'light',\n",
    "    'S4': 'heavy'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r'data\\wikihow_summary_injection_PT_v2_scorer_v2\\anthropic_claude-3-5-haiku-20241022\\dataset_typo_rates_injected.csv')\n",
    "\n",
    "# Display basic info about the DataFrame\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualizer import VisualisationConfig, visualize\n",
    "import altair as alt\n",
    "\n",
    "visualize(\n",
    "    evals_analyzer.df,\n",
    "    VisualisationConfig(\n",
    "        plot_fn=alt.Chart.mark_bar,\n",
    "        x_category=\"model\",\n",
    "        y_category=\"mean(score_custom_match_accuracy)\",\n",
    "        h_concat_category=\"treatment_type\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualizer import VisualisationConfig, visualize\n",
    "import altair as alt\n",
    "\n",
    "visualize(\n",
    "    evals_analyzer.df,\n",
    "    VisualisationConfig(\n",
    "        plot_fn=alt.Chart.mark_bar,\n",
    "        x_category=\"model\",\n",
    "        y_category=\"mean(score_custom_prompt_criterion_mgf_accuracy)\",\n",
    "        h_concat_category=\"treatment_type\",\n",
    "        v_concat_category=\"treatment_strength\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualizer import VisualisationConfig, visualize\n",
    "import altair as alt\n",
    "\n",
    "visualize(\n",
    "    evals_analyzer.df,\n",
    "    VisualisationConfig(\n",
    "        plot_fn=alt.Chart.mark_bar,\n",
    "        x_category=\"treatment_strength\",\n",
    "        y_category=\"mean(score_custom_prompt_criterion_mgf_accuracy)\",\n",
    "        h_concat_category=\"treatment_type\",\n",
    "        v_concat_category=\"model\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(\n",
    "    evals_analyzer.df,\n",
    "    VisualisationConfig(\n",
    "        plot_fn=alt.Chart.mark_bar,\n",
    "        x_category=\"injection_length\",\n",
    "        y_category=\"mean(score_custom_prompt_criterion_mgf_accuracy)\",\n",
    "        h_concat_category=\"treatment_type\",\n",
    "        v_concat_category=\"model\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_analyzer.df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_analyzer.df[\"injection_length\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
