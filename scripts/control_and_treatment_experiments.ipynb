{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inspect_helpers.tasks import injection_consistency_and_recognition\n",
    "from inspect_ai.log import EvalLog, list_eval_logs, read_eval_log\n",
    "from inspect_ai import eval\n",
    "from src.data_structures import ExperimentConfig, ControlConfig\n",
    "from src.inspect_helpers.datasets import ROW_INDEX_KEY\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"wikihow_summary_injection\"\n",
    "LOG_DIR = f\"logs/{EXPERIMENT_NAME}/control\"\n",
    "\n",
    "MODELS = [\n",
    "    # \"anthropic/claude-sonnet-4-20250514\",\n",
    "    \"anthropic/claude-3-5-haiku-20241022\",\n",
    "    # \"ollama/gemma3:1b-it-q8_0\",\n",
    "    # \"ollama/llama3.2:1b-instruct-q8_0\"\n",
    "]\n",
    "\n",
    "SCORING_MODELS = [\n",
    "    \"together_ai/Qwen3-235B-A22B\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE_LOCAL = 4\n",
    "MAX_CONNECTIONS_API = 100\n",
    "LIMIT  = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/wikihow.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m experiment_config = ExperimentConfig(\n\u001b[32m      2\u001b[39m     control=ControlConfig(file_name=\u001b[33m\"\u001b[39m\u001b[33mdata/wikihow.csv\u001b[39m\u001b[33m\"\u001b[39m, scorer_criteria=[\u001b[33m\"\u001b[39m\u001b[33mNo\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m )\n\u001b[32m      5\u001b[39m \u001b[38;5;28meval\u001b[39m(\n\u001b[32m      6\u001b[39m     tasks=[\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         \u001b[43minjection_consistency_and_recognition\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtreatment_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscorer_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     ],\n\u001b[32m     13\u001b[39m     model=MODELS,\n\u001b[32m     14\u001b[39m     limit=LIMIT,\n\u001b[32m     15\u001b[39m     log_dir=LOG_DIR,\n\u001b[32m     16\u001b[39m     max_connections=\u001b[32m100\u001b[39m,\n\u001b[32m     17\u001b[39m     timeout=\u001b[32m500\u001b[39m,\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/injection_recognition/lib/python3.11/site-packages/inspect_ai/_eval/registry.py:122\u001b[39m, in \u001b[36mtask.<locals>.create_task_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*w_args, **w_kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(task_type)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*w_args: Any, **w_kwargs: Any) -> Task:\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# Create the task\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     task_instance = \u001b[43mtask_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mw_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mw_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# Tag the task with registry information\u001b[39;00m\n\u001b[32m    125\u001b[39m     registry_tag(\n\u001b[32m    126\u001b[39m         task_type,\n\u001b[32m    127\u001b[39m         task_instance,\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m         **w_kwargs,\n\u001b[32m    135\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/injection-recognition/src/inspect_helpers/tasks.py:29\u001b[39m, in \u001b[36minjection_consistency_and_recognition\u001b[39m\u001b[34m(csv_file_path, treatment_col, scorer_criteria, default_prefill, prompt_template_path, prefill_template_path, passage_column)\u001b[39m\n\u001b[32m     25\u001b[39m         \u001b[38;5;167;01mWarning\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMatch scorer criteria expects \u001b[39m\u001b[33m'\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNo\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscorer_criteria\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Converting to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch_criteria\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m         scorer_criteria = match_criteria\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Task(\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     dataset=\u001b[43mcreate_samples_from_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtreatment_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtreatment_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_template_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_template_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefill_template_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefill_template_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpassage_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassage_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     36\u001b[39m     solver=prefill_generate(default_prefill),\n\u001b[32m     37\u001b[39m     scorer=[\n\u001b[32m     38\u001b[39m         custom_match(target=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscorer_criteria[\u001b[32m0\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(scorer_criteria,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mmatch_criteria\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, location=\u001b[33m\"\u001b[39m\u001b[33many\u001b[39m\u001b[33m\"\u001b[39m, ignore_case=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m     39\u001b[39m         custom_prompt_criterion_mgf(criterion=scorer_criteria[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scorer_criteria, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m scorer_criteria),\n\u001b[32m     40\u001b[39m     ],\n\u001b[32m     41\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/injection-recognition/src/inspect_helpers/datasets.py:75\u001b[39m, in \u001b[36mcreate_samples_from_csv\u001b[39m\u001b[34m(csv_file_path, treatment_col, passage_column, prompt_template_path, prefill_template_path)\u001b[39m\n\u001b[32m     72\u001b[39m prefill_template = load_prefill_template(prefill_template_path)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Read CSV data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     76\u001b[39m     reader = csv.DictReader(f)\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m row_idx, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reader):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/wikihow.csv'"
     ]
    }
   ],
   "source": [
    "experiment_config = ExperimentConfig(\n",
    "    control=ControlConfig(file_name=\"data/wikihow.csv\", scorer_criteria=[\"No\", \"None\"])\n",
    ")\n",
    "\n",
    "eval(\n",
    "    tasks=[\n",
    "        injection_consistency_and_recognition(\n",
    "            csv_file_path=experiment_config.control.file_name,\n",
    "            treatment_col=None,\n",
    "            scorer_criteria=experiment_config.control.scorer_criteria,\n",
    "        )\n",
    "    ],\n",
    "    model=MODELS,\n",
    "    limit=LIMIT,\n",
    "    log_dir=LOG_DIR,\n",
    "    max_connections=100,\n",
    "    timeout=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make CSVs from the control eval logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks on evaluation logs...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Multiple successful logs found for some models:\n\nModel 'anthropic_claude-3-5-haiku-20241022' has 4 successful logs in experiment 'wikihow_summary_injection':\n  Files: file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-47-43+01-00_injection-consistency-and-recognition_iWtGUwcP6d5mcgkmxWX8ri.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-44-29+01-00_injection-consistency-and-recognition_9QSZuG8RVQfUWN9ThZqwPi.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-43-38+01-00_injection-consistency-and-recognition_isS4ZeATVTWA52JFqKsGuL.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-11T19-23-46+01-00_injection-consistency-and-recognition_FT6CRXymD8Mz7Rvjmd9TWK.eval\n  Please remove duplicate logs or use only one successful run per model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Run sanity checks first\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning sanity checks on evaluation logs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m logs_by_model = \u001b[43mcheck_log_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOG_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXPERIMENT_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Sanity checks passed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Process each successful evaluation log\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mcheck_log_sanity\u001b[39m\u001b[34m(log_dir, experiment_name)\u001b[39m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     46\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no successful logs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m         )\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMultiple successful logs found for some models:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(errors)\n\u001b[32m     52\u001b[39m     )\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logs_by_model\n",
      "\u001b[31mValueError\u001b[39m: Multiple successful logs found for some models:\n\nModel 'anthropic_claude-3-5-haiku-20241022' has 4 successful logs in experiment 'wikihow_summary_injection':\n  Files: file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-47-43+01-00_injection-consistency-and-recognition_iWtGUwcP6d5mcgkmxWX8ri.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-44-29+01-00_injection-consistency-and-recognition_9QSZuG8RVQfUWN9ThZqwPi.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-43-38+01-00_injection-consistency-and-recognition_isS4ZeATVTWA52JFqKsGuL.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-11T19-23-46+01-00_injection-consistency-and-recognition_FT6CRXymD8Mz7Rvjmd9TWK.eval\n  Please remove duplicate logs or use only one successful run per model."
     ]
    }
   ],
   "source": [
    "def check_log_sanity(log_dir, experiment_name=None):\n",
    "    \"\"\"\n",
    "    Check that there's only one successful log per model in the log directory.\n",
    "\n",
    "    Args:\n",
    "        log_dir: Directory containing evaluation logs\n",
    "        experiment_name: Optional experiment name for better error messages\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If multiple successful logs found for the same model\n",
    "    \"\"\"\n",
    "    logs_by_model = {}\n",
    "\n",
    "    # Collect all logs and group by model\n",
    "    for eval_log_info in list_eval_logs(log_dir):\n",
    "        eval_log = read_eval_log(eval_log_info)\n",
    "        model_name = eval_log.eval.model.replace(\"/\", \"_\")\n",
    "\n",
    "        if model_name not in logs_by_model:\n",
    "            logs_by_model[model_name] = []\n",
    "\n",
    "        logs_by_model[model_name].append(\n",
    "            {\"log_info\": eval_log_info, \"eval_log\": eval_log, \"status\": eval_log.status}\n",
    "        )\n",
    "\n",
    "    # Check for multiple successful logs per model\n",
    "    errors = []\n",
    "    for model_name, logs in logs_by_model.items():\n",
    "        successful_logs = [log for log in logs if log[\"status\"] == \"success\"]\n",
    "\n",
    "        if len(successful_logs) > 1:\n",
    "            experiment_prefix = (\n",
    "                f\" in experiment '{experiment_name}'\" if experiment_name else \"\"\n",
    "            )\n",
    "            log_files = [log[\"log_info\"].name for log in successful_logs]\n",
    "            errors.append(\n",
    "                f\"Model '{model_name}' has {len(successful_logs)} successful logs{experiment_prefix}:\\n\"\n",
    "                f\"  Files: {', '.join(log_files)}\\n\"\n",
    "                f\"  Please remove duplicate logs or use only one successful run per model.\"\n",
    "            )\n",
    "        elif len(successful_logs) == 0:\n",
    "            experiment_prefix = (\n",
    "                f\" in experiment '{experiment_name}'\" if experiment_name else \"\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Warning: Model '{model_name}' has no successful logs{experiment_prefix}\"\n",
    "            )\n",
    "\n",
    "    if errors:\n",
    "        raise ValueError(\n",
    "            \"Multiple successful logs found for some models:\\n\\n\" + \"\\n\\n\".join(errors)\n",
    "        )\n",
    "\n",
    "    return logs_by_model\n",
    "\n",
    "\n",
    "def extract_responses_to_csv(\n",
    "    eval_log: EvalLog,\n",
    "    original_csv_path,\n",
    "    output_csv_path,\n",
    "    response_column_name=\"model_response\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract model responses from eval log and save to CSV with responses mapped back to original rows.\n",
    "\n",
    "    Args:\n",
    "        eval_log: The evaluation log containing samples and responses\n",
    "        original_csv_path: Path to the original CSV file\n",
    "        output_csv_path: Path where to save the CSV with responses\n",
    "        response_column_name: Name of the column to add with model responses\n",
    "    \"\"\"\n",
    "    # Load original CSV\n",
    "    df = pd.read_csv(original_csv_path)\n",
    "\n",
    "    # Initialize the response column with empty strings\n",
    "    df[response_column_name] = \"\"\n",
    "\n",
    "    # Extract responses from samples\n",
    "    if eval_log.samples:\n",
    "        for sample in eval_log.samples:\n",
    "            # Get the row index from metadata\n",
    "            row_index = sample.metadata.get(ROW_INDEX_KEY)\n",
    "            if row_index is not None and row_index < len(df):\n",
    "                # Extract the model response\n",
    "                if sample.output and sample.output.message:\n",
    "                    model_response = sample.output.message.content\n",
    "                    if isinstance(model_response, list):\n",
    "                        # If content is a list, join text parts\n",
    "                        model_response = \"\".join(\n",
    "                            [\n",
    "                                part.text\n",
    "                                for part in model_response\n",
    "                                if hasattr(part, \"text\")\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                    model_response = (\n",
    "                        model_response.split(\"Task 2:\")[0]\n",
    "                        .strip()\n",
    "                        .split(\"Task 1:\")[1]\n",
    "                        .strip()\n",
    "                    )\n",
    "                    df.loc[row_index, response_column_name] = model_response\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "\n",
    "    # Save the CSV with responses\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Saved CSV with responses to: {output_csv_path}\")\n",
    "\n",
    "\n",
    "# Run sanity checks first\n",
    "print(\"Running sanity checks on evaluation logs...\")\n",
    "logs_by_model = check_log_sanity(LOG_DIR, EXPERIMENT_NAME)\n",
    "print(\"✓ Sanity checks passed!\")\n",
    "\n",
    "# Process each successful evaluation log\n",
    "for model_name, logs in logs_by_model.items():\n",
    "    # Find the successful log for this model\n",
    "    successful_logs = [log for log in logs if log[\"status\"] == \"success\"]\n",
    "\n",
    "    if len(successful_logs) == 1:\n",
    "        eval_log = successful_logs[0][\"eval_log\"]\n",
    "\n",
    "        # Create output path: data/experiment_name/model_name/dataset.csv\n",
    "        output_csv_path = os.path.join(\n",
    "            f\"data/{EXPERIMENT_NAME}\", model_name, \"dataset.csv\"\n",
    "        )\n",
    "\n",
    "        # Extract responses and save to CSV\n",
    "        extract_responses_to_csv(\n",
    "            eval_log=eval_log,\n",
    "            original_csv_path=experiment_config.control.file_name,\n",
    "            output_csv_path=output_csv_path,\n",
    "            response_column_name=\"model_summary\",\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping model '{model_name}' - no successful logs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying treatments to csv datasets (Jesse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop though the subdirs in the data/{EXPERIMENT_NAME} dir and take the dataset.csv files and apply treatments to them resulting in {treatment_name}_treatment.csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inspect_helpers.tasks import injection_consistency_and_recognition\n",
    "from inspect_ai import eval\n",
    "\n",
    "treatment_csv_path = \"data/wikisum_capitalization_treatments_6_10.csv\"\n",
    "treatment_log_dir = \"logs/wikisum_capitalization_treatments\"\n",
    "\n",
    "eval(\n",
    "    tasks=[\n",
    "        injection_consistency_and_recognition(\n",
    "            csv_file_path=treatment_csv_path,\n",
    "            treatment_col=treatment_col,\n",
    "        )\n",
    "        for treatment_col in [\"IL33_S1\", \"IL33_S2\"]\n",
    "    ],\n",
    "    model=[\"ollama/qwen3:0.6b\", \"anthropic/claude-sonnet-4-20250514\"],\n",
    "    limit=2,\n",
    "    log_dir=treatment_log_dir,\n",
    "    max_connections=100,\n",
    "    timeout=500,\n",
    ")\n",
    "\n",
    "# Process treatment evaluation logs with sanity checks\n",
    "process_all_logs_to_csv(\n",
    "    log_dir=treatment_log_dir,\n",
    "    original_csv_path=treatment_csv_path,\n",
    "    experiment_name=\"wikisum_capitalization_treatments\",\n",
    "    response_column_name=\"model_response\",\n",
    "    check_sanity=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_logs_to_csv(\n",
    "    log_dir,\n",
    "    original_csv_path,\n",
    "    experiment_name=None,\n",
    "    response_column_name=\"model_response\",\n",
    "    check_sanity=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process all evaluation logs in a directory and save CSVs with model responses.\n",
    "\n",
    "    Args:\n",
    "        log_dir: Directory containing evaluation logs\n",
    "        original_csv_path: Path to the original CSV file\n",
    "        experiment_name: Optional experiment name for organizing output\n",
    "        response_column_name: Name of the column to add with model responses\n",
    "        check_sanity: Whether to run sanity checks on logs (default: True)\n",
    "    \"\"\"\n",
    "    if check_sanity:\n",
    "        print(f\"Running sanity checks on logs in {log_dir}...\")\n",
    "        logs_by_model = check_log_sanity(log_dir, experiment_name)\n",
    "        print(\"✓ Sanity checks passed!\")\n",
    "\n",
    "        # Process only successful logs\n",
    "        for model_name, logs in logs_by_model.items():\n",
    "            successful_logs = [log for log in logs if log[\"status\"] == \"success\"]\n",
    "\n",
    "            for log_data in successful_logs:\n",
    "                eval_log = log_data[\"eval_log\"]\n",
    "\n",
    "                # Get treatment column if it exists\n",
    "                treatment_col = None\n",
    "                if eval_log.samples and len(eval_log.samples) > 0:\n",
    "                    treatment_col = eval_log.samples[0].metadata.get(\"treatment_column\")\n",
    "\n",
    "                # Create output path structure\n",
    "                dataset_name = os.path.basename(original_csv_path)\n",
    "                if experiment_name:\n",
    "                    base_dir = f\"logs/{experiment_name}\"\n",
    "                else:\n",
    "                    base_dir = log_dir\n",
    "\n",
    "                if treatment_col:\n",
    "                    output_csv_path = os.path.join(\n",
    "                        base_dir, model_name, treatment_col, dataset_name\n",
    "                    )\n",
    "                else:\n",
    "                    output_csv_path = os.path.join(base_dir, model_name, dataset_name)\n",
    "\n",
    "                # Extract responses and save to CSV\n",
    "                extract_responses_to_csv(\n",
    "                    eval_log=eval_log,\n",
    "                    original_csv_path=original_csv_path,\n",
    "                    output_csv_path=output_csv_path,\n",
    "                    response_column_name=response_column_name,\n",
    "                )\n",
    "    else:\n",
    "        # Original behavior - process all logs without sanity checks\n",
    "        for eval_log_info in list_eval_logs(log_dir):\n",
    "            eval_log = read_eval_log(eval_log_info)\n",
    "\n",
    "            # Extract model name\n",
    "            model_name = eval_log.eval.model.replace(\"/\", \"_\")\n",
    "\n",
    "            # Get treatment column if it exists\n",
    "            treatment_col = None\n",
    "            if eval_log.samples and len(eval_log.samples) > 0:\n",
    "                treatment_col = eval_log.samples[0].metadata.get(\"treatment_column\")\n",
    "\n",
    "            # Create output path structure\n",
    "            dataset_name = os.path.basename(original_csv_path)\n",
    "            if experiment_name:\n",
    "                base_dir = f\"logs/{experiment_name}\"\n",
    "            else:\n",
    "                base_dir = log_dir\n",
    "\n",
    "            if treatment_col:\n",
    "                output_csv_path = os.path.join(\n",
    "                    base_dir, model_name, treatment_col, dataset_name\n",
    "                )\n",
    "            else:\n",
    "                output_csv_path = os.path.join(base_dir, model_name, dataset_name)\n",
    "\n",
    "            # Extract responses and save to CSV\n",
    "            extract_responses_to_csv(\n",
    "                eval_log=eval_log,\n",
    "                original_csv_path=original_csv_path,\n",
    "                output_csv_path=output_csv_path,\n",
    "                response_column_name=response_column_name,\n",
    "            )\n",
    "\n",
    "\n",
    "# Example usage for any experiment:\n",
    "# process_all_logs_to_csv(\"logs/your_experiment\", \"data/your_data.csv\", \"your_experiment_name\")\n",
    "#\n",
    "# To skip sanity checks (not recommended for control experiments):\n",
    "# process_all_logs_to_csv(\"logs/your_experiment\", \"data/your_data.csv\", check_sanity=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "injection_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
