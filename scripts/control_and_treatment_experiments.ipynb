{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inspect_helpers.tasks import injection_consistency_and_recognition\n",
    "from inspect_ai.log import EvalLog, list_eval_logs, read_eval_log\n",
    "from inspect_ai import eval\n",
    "from src.data_structures import ExperimentConfig, ControlConfig\n",
    "from src.inspect_helpers.datasets import ROW_INDEX_KEY\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"wikihow_summary_injection\"\n",
    "LOG_DIR = f\"logs/{EXPERIMENT_NAME}/control\"\n",
    "\n",
    "MODELS = [\n",
    "    \"anthropic/claude-sonnet-4-20250514\",\n",
    "    \"anthropic/claude-3-5-haiku-20241022\",\n",
    "    \"ollama/gemma3:1b-it-q8_0\",\n",
    "    \"ollama/llama3.2:1b-instruct-q8_0\"\n",
    "]\n",
    "\n",
    "SCORING_MODELS = [\n",
    "    \"together_ai/Qwen3-235B-A22B\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE_LOCAL = 4\n",
    "MAX_CONNECTIONS_API = 100\n",
    "LIMIT  = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df0029c523846aaa14653f635abf73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_config = ExperimentConfig(\n",
    "    control=ControlConfig(file_name=\"data/wikihow.csv\", scorer_criteria=[\"No\"])\n",
    ")\n",
    "\n",
    "eval(\n",
    "    tasks=[\n",
    "        injection_consistency_and_recognition(\n",
    "            csv_file_path=experiment_config.control.file_name,\n",
    "        )\n",
    "    ],\n",
    "    model=MODELS,\n",
    "    limit=LIMIT,\n",
    "    log_dir=LOG_DIR,\n",
    "    max_connections=100,\n",
    "    timeout=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make CSVs from the control eval logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks on evaluation logs...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Multiple successful logs found for some models:\n\nModel 'anthropic_claude-3-5-haiku-20241022' has 4 successful logs in experiment 'wikihow_summary_injection':\n  Files: file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-47-43+01-00_injection-consistency-and-recognition_iWtGUwcP6d5mcgkmxWX8ri.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-44-29+01-00_injection-consistency-and-recognition_9QSZuG8RVQfUWN9ThZqwPi.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-43-38+01-00_injection-consistency-and-recognition_isS4ZeATVTWA52JFqKsGuL.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-11T19-23-46+01-00_injection-consistency-and-recognition_FT6CRXymD8Mz7Rvjmd9TWK.eval\n  Please remove duplicate logs or use only one successful run per model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Run sanity checks first\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning sanity checks on evaluation logs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m logs_by_model = \u001b[43mcheck_log_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOG_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEXPERIMENT_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Sanity checks passed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Process each successful evaluation log\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mcheck_log_sanity\u001b[39m\u001b[34m(log_dir, experiment_name)\u001b[39m\n\u001b[32m     45\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     46\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no successful logs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m         )\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMultiple successful logs found for some models:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(errors)\n\u001b[32m     52\u001b[39m     )\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logs_by_model\n",
      "\u001b[31mValueError\u001b[39m: Multiple successful logs found for some models:\n\nModel 'anthropic_claude-3-5-haiku-20241022' has 4 successful logs in experiment 'wikihow_summary_injection':\n  Files: file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-47-43+01-00_injection-consistency-and-recognition_iWtGUwcP6d5mcgkmxWX8ri.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-44-29+01-00_injection-consistency-and-recognition_9QSZuG8RVQfUWN9ThZqwPi.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-12T09-43-38+01-00_injection-consistency-and-recognition_isS4ZeATVTWA52JFqKsGuL.eval, file:///Users/work/injection-recognition/logs/wikihow_summary_injection/control/2025-07-11T19-23-46+01-00_injection-consistency-and-recognition_FT6CRXymD8Mz7Rvjmd9TWK.eval\n  Please remove duplicate logs or use only one successful run per model."
     ]
    }
   ],
   "source": [
    "def check_log_sanity(log_dir, experiment_name=None):\n",
    "    \"\"\"\n",
    "    Check that there's only one successful log per model in the log directory.\n",
    "\n",
    "    Args:\n",
    "        log_dir: Directory containing evaluation logs\n",
    "        experiment_name: Optional experiment name for better error messages\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If multiple successful logs found for the same model\n",
    "    \"\"\"\n",
    "    logs_by_model = {}\n",
    "\n",
    "    # Collect all logs and group by model\n",
    "    for eval_log_info in list_eval_logs(log_dir):\n",
    "        eval_log = read_eval_log(eval_log_info)\n",
    "        model_name = eval_log.eval.model.replace(\"/\", \"_\")\n",
    "\n",
    "        if model_name not in logs_by_model:\n",
    "            logs_by_model[model_name] = []\n",
    "\n",
    "        logs_by_model[model_name].append(\n",
    "            {\"log_info\": eval_log_info, \"eval_log\": eval_log, \"status\": eval_log.status}\n",
    "        )\n",
    "\n",
    "    # Check for multiple successful logs per model\n",
    "    errors = []\n",
    "    for model_name, logs in logs_by_model.items():\n",
    "        successful_logs = [log for log in logs if log[\"status\"] == \"success\"]\n",
    "\n",
    "        if len(successful_logs) > 1:\n",
    "            experiment_prefix = (\n",
    "                f\" in experiment '{experiment_name}'\" if experiment_name else \"\"\n",
    "            )\n",
    "            log_files = [log[\"log_info\"].name for log in successful_logs]\n",
    "            errors.append(\n",
    "                f\"Model '{model_name}' has {len(successful_logs)} successful logs{experiment_prefix}:\\n\"\n",
    "                f\"  Files: {', '.join(log_files)}\\n\"\n",
    "                f\"  Please remove duplicate logs or use only one successful run per model.\"\n",
    "            )\n",
    "        elif len(successful_logs) == 0:\n",
    "            experiment_prefix = (\n",
    "                f\" in experiment '{experiment_name}'\" if experiment_name else \"\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Warning: Model '{model_name}' has no successful logs{experiment_prefix}\"\n",
    "            )\n",
    "\n",
    "    if errors:\n",
    "        raise ValueError(\n",
    "            \"Multiple successful logs found for some models:\\n\\n\" + \"\\n\\n\".join(errors)\n",
    "        )\n",
    "\n",
    "    return logs_by_model\n",
    "\n",
    "\n",
    "def extract_responses_to_csv(\n",
    "    eval_log: EvalLog,\n",
    "    original_csv_path,\n",
    "    output_csv_path,\n",
    "    response_column_name=\"model_response\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract model responses from eval log and save to CSV with responses mapped back to original rows.\n",
    "\n",
    "    Args:\n",
    "        eval_log: The evaluation log containing samples and responses\n",
    "        original_csv_path: Path to the original CSV file\n",
    "        output_csv_path: Path where to save the CSV with responses\n",
    "        response_column_name: Name of the column to add with model responses\n",
    "    \"\"\"\n",
    "    # Load original CSV\n",
    "    df = pd.read_csv(original_csv_path)\n",
    "\n",
    "    # Initialize the response column with empty strings\n",
    "    df[response_column_name] = \"\"\n",
    "\n",
    "    # Extract responses from samples\n",
    "    if eval_log.samples:\n",
    "        for sample in eval_log.samples:\n",
    "            # Get the row index from metadata\n",
    "            row_index = sample.metadata.get(ROW_INDEX_KEY)\n",
    "            if row_index is not None and row_index < len(df):\n",
    "                # Extract the model response\n",
    "                if sample.output and sample.output.message:\n",
    "                    model_response = sample.output.message.content\n",
    "                    if isinstance(model_response, list):\n",
    "                        # If content is a list, join text parts\n",
    "                        model_response = \"\".join(\n",
    "                            [\n",
    "                                part.text\n",
    "                                for part in model_response\n",
    "                                if hasattr(part, \"text\")\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                    model_response = (\n",
    "                        model_response.split(\"Task 2:\")[0]\n",
    "                        .strip()\n",
    "                        .split(\"Task 1:\")[1]\n",
    "                        .strip()\n",
    "                    )\n",
    "                    df.loc[row_index, response_column_name] = model_response\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "\n",
    "    # Save the CSV with responses\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Saved CSV with responses to: {output_csv_path}\")\n",
    "\n",
    "\n",
    "# Run sanity checks first\n",
    "print(\"Running sanity checks on evaluation logs...\")\n",
    "logs_by_model = check_log_sanity(LOG_DIR, EXPERIMENT_NAME)\n",
    "print(\"âœ“ Sanity checks passed!\")\n",
    "\n",
    "# Process each successful evaluation log\n",
    "for model_name, logs in logs_by_model.items():\n",
    "    # Find the successful log for this model\n",
    "    successful_logs = [log for log in logs if log[\"status\"] == \"success\"]\n",
    "\n",
    "    if len(successful_logs) == 1:\n",
    "        eval_log = successful_logs[0][\"eval_log\"]\n",
    "\n",
    "        # Create output path: data/experiment_name/model_name/dataset.csv\n",
    "        output_csv_path = os.path.join(\n",
    "            f\"data/{EXPERIMENT_NAME}\", model_name, \"dataset.csv\"\n",
    "        )\n",
    "\n",
    "        # Extract responses and save to CSV\n",
    "        extract_responses_to_csv(\n",
    "            eval_log=eval_log,\n",
    "            original_csv_path=experiment_config.control.file_name,\n",
    "            output_csv_path=output_csv_path,\n",
    "            response_column_name=\"model_summary\",\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping model '{model_name}' - no successful logs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying treatments to csv datasets (Jesse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop though the subdirs in the data/{EXPERIMENT_NAME} dir and take the dataset.csv files and apply treatments to them resulting in {treatment_name}_treatment.csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treatment Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inspect_helpers.tasks import injection_consistency_and_recognition\n",
    "from inspect_ai import eval\n",
    "\n",
    "treatment_csv_path = \"data/wikisum_capitalization_treatments_6_10.csv\"\n",
    "treatment_log_dir = \"logs/wikisum_capitalization_treatments\"\n",
    "\n",
    "eval(\n",
    "    tasks=[\n",
    "        injection_consistency_and_recognition(\n",
    "            csv_file_path=treatment_csv_path,\n",
    "            treatment_col=treatment_col,\n",
    "        )\n",
    "        for treatment_col in [\"IL33_S1\", \"IL33_S2\"]\n",
    "    ],\n",
    "    model=[\"ollama/qwen3:0.6b\", \"anthropic/claude-sonnet-4-20250514\"],\n",
    "    limit=2,\n",
    "    log_dir=treatment_log_dir,\n",
    "    max_connections=100,\n",
    "    timeout=500,\n",
    ")\n",
    "\n",
    "# Process treatment evaluation logs with sanity checks\n",
    "process_all_logs_to_csv(\n",
    "    log_dir=treatment_log_dir,\n",
    "    original_csv_path=treatment_csv_path,\n",
    "    experiment_name=\"wikisum_capitalization_treatments\",\n",
    "    response_column_name=\"model_response\",\n",
    "    check_sanity=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_logs_to_csv(\n",
    "    log_dir,\n",
    "    original_csv_path,\n",
    "    experiment_name=None,\n",
    "    response_column_name=\"model_response\",\n",
    "    check_sanity=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process all evaluation logs in a directory and save CSVs with model responses.\n",
    "\n",
    "    Args:\n",
    "        log_dir: Directory containing evaluation logs\n",
    "        original_csv_path: Path to the original CSV file\n",
    "        experiment_name: Optional experiment name for organizing output\n",
    "        response_column_name: Name of the column to add with model responses\n",
    "        check_sanity: Whether to run sanity checks on logs (default: True)\n",
    "    \"\"\"\n",
    "    if check_sanity:\n",
    "        print(f\"Running sanity checks on logs in {log_dir}...\")\n",
    "        logs_by_model = check_log_sanity(log_dir, experiment_name)\n",
    "        print(\"âœ“ Sanity checks passed!\")\n",
    "\n",
    "        # Process only successful logs\n",
    "        for model_name, logs in logs_by_model.items():\n",
    "            successful_logs = [log for log in logs if log[\"status\"] == \"success\"]\n",
    "\n",
    "            for log_data in successful_logs:\n",
    "                eval_log = log_data[\"eval_log\"]\n",
    "\n",
    "                # Get treatment column if it exists\n",
    "                treatment_col = None\n",
    "                if eval_log.samples and len(eval_log.samples) > 0:\n",
    "                    treatment_col = eval_log.samples[0].metadata.get(\"treatment_column\")\n",
    "\n",
    "                # Create output path structure\n",
    "                dataset_name = os.path.basename(original_csv_path)\n",
    "                if experiment_name:\n",
    "                    base_dir = f\"logs/{experiment_name}\"\n",
    "                else:\n",
    "                    base_dir = log_dir\n",
    "\n",
    "                if treatment_col:\n",
    "                    output_csv_path = os.path.join(\n",
    "                        base_dir, model_name, treatment_col, dataset_name\n",
    "                    )\n",
    "                else:\n",
    "                    output_csv_path = os.path.join(base_dir, model_name, dataset_name)\n",
    "\n",
    "                # Extract responses and save to CSV\n",
    "                extract_responses_to_csv(\n",
    "                    eval_log=eval_log,\n",
    "                    original_csv_path=original_csv_path,\n",
    "                    output_csv_path=output_csv_path,\n",
    "                    response_column_name=response_column_name,\n",
    "                )\n",
    "    else:\n",
    "        # Original behavior - process all logs without sanity checks\n",
    "        for eval_log_info in list_eval_logs(log_dir):\n",
    "            eval_log = read_eval_log(eval_log_info)\n",
    "\n",
    "            # Extract model name\n",
    "            model_name = eval_log.eval.model.replace(\"/\", \"_\")\n",
    "\n",
    "            # Get treatment column if it exists\n",
    "            treatment_col = None\n",
    "            if eval_log.samples and len(eval_log.samples) > 0:\n",
    "                treatment_col = eval_log.samples[0].metadata.get(\"treatment_column\")\n",
    "\n",
    "            # Create output path structure\n",
    "            dataset_name = os.path.basename(original_csv_path)\n",
    "            if experiment_name:\n",
    "                base_dir = f\"logs/{experiment_name}\"\n",
    "            else:\n",
    "                base_dir = log_dir\n",
    "\n",
    "            if treatment_col:\n",
    "                output_csv_path = os.path.join(\n",
    "                    base_dir, model_name, treatment_col, dataset_name\n",
    "                )\n",
    "            else:\n",
    "                output_csv_path = os.path.join(base_dir, model_name, dataset_name)\n",
    "\n",
    "            # Extract responses and save to CSV\n",
    "            extract_responses_to_csv(\n",
    "                eval_log=eval_log,\n",
    "                original_csv_path=original_csv_path,\n",
    "                output_csv_path=output_csv_path,\n",
    "                response_column_name=response_column_name,\n",
    "            )\n",
    "\n",
    "\n",
    "# Example usage for any experiment:\n",
    "# process_all_logs_to_csv(\"logs/your_experiment\", \"data/your_data.csv\", \"your_experiment_name\")\n",
    "#\n",
    "# To skip sanity checks (not recommended for control experiments):\n",
    "# process_all_logs_to_csv(\"logs/your_experiment\", \"data/your_data.csv\", check_sanity=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "injection_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
